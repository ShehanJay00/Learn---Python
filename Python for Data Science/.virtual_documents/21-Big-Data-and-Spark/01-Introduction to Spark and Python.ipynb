


from pyspark import SparkContext





sc = SparkContext()








%%writefile example.txt
first line
second line
third line
fourth line








textFile = sc.textFile('example.txt')





textFile.count()


textFile.first()





secfind = textFile.filter(lambda line: 'second' in line)


# RDD
secfind


# Perform action on transformation
secfind.collect()


# Perform action on transformation
secfind.count()



