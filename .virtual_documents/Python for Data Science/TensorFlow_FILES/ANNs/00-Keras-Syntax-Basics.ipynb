








import pandas as pd


df = pd.read_csv('../DATA/fake_reg.csv')


df.head()





import seaborn as sns
import matplotlib.pyplot as plt


sns.pairplot(df)








from sklearn.model_selection import train_test_split


# Convert Pandas to Numpy for Keras

# Features
X = df[['feature1','feature2']].values

# Label
y = df['price'].values

# Split
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=42)


X_train.shape


X_test.shape


y_train.shape


y_test.shape





from sklearn.preprocessing import MinMaxScaler


help(MinMaxScaler)


scaler = MinMaxScaler()


# Notice to prevent data leakage from the test set, we only fit our scaler to the training set


scaler.fit(X_train)


X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)








import tensorflow as tf


from tensorflow.keras.models import Sequential


help(Sequential)





from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation





model = Sequential([
    Dense(units=2),
    Dense(units=2),
    Dense(units=2)
])





model = Sequential()

model.add(Dense(2))
model.add(Dense(2))
model.add(Dense(2))





model = Sequential()

model.add(Dense(4,activation='relu'))
model.add(Dense(4,activation='relu'))
model.add(Dense(4,activation='relu'))

# Final output node for prediction
model.add(Dense(1))

model.compile(optimizer='rmsprop',loss='mse')

# RMSprop stands for "Root Mean Square Propagation."


### Choosing an optimizer and loss

Keep in mind what kind of problem you are trying to solve:

    # For a multi-class classification problem
    model.compile(optimizer='rmsprop',
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])

    # For a binary classification problem
    model.compile(optimizer='rmsprop',
                  loss='binary_crossentropy',
                  metrics=['accuracy'])

    # For a mean squared error regression problem
    model.compile(optimizer='rmsprop',
                  loss='mse')





model.fit(X_train,y_train,epochs=250)





model.history.history


loss = model.history.history['loss']


sns.lineplot(x=range(len(loss)),y=loss)
plt.title("Training Loss per Epoch");





model.metrics_names


training_score = model.evaluate(X_train,y_train,verbose=0)
test_score = model.evaluate(X_test,y_test,verbose=0)





training_score


test_score





test_predictions = model.predict(X_test)


test_predictions


pred_df = pd.DataFrame(y_test,columns=['Test Y'])


pred_df


test_predictions = pd.Series(test_predictions.reshape(300,))


test_predictions


pred_df = pd.concat([pred_df,test_predictions],axis=1)


pred_df.columns = ['Test Y','Model Predictions']


pred_df





sns.scatterplot(x='Test Y',y='Model Predictions',data=pred_df)


pred_df['Error'] = pred_df['Test Y'] - pred_df['Model Predictions']


sns.distplot(pred_df['Error'],bins=50)


from sklearn.metrics import mean_absolute_error,mean_squared_error


mean_absolute_error(pred_df['Test Y'],pred_df['Model Predictions'])


mean_squared_error(pred_df['Test Y'],pred_df['Model Predictions'])


# Essentially the same thing, difference just due to precision
test_score


#RMSE
test_score**0.5





# [[Feature1, Feature2]]
new_gem = [[998,1000]]


# Don't forget to scale!
scaler.transform(new_gem)


new_gem = scaler.transform(new_gem)


model.predict(new_gem)





from tensorflow.keras.models import load_model


model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'


later_model = load_model('Python for Data Science/TensorFlow_FILES/ANNs/my_model.h5')


later_model.predict(new_gem)


model.save('my_model', save_format='tf')  # Save as TensorFlow SavedModel




